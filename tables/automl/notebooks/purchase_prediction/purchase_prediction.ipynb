{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "colab_C4M.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "OFJAWue1ss3C",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#Purchase Prediction with AutoML Tables\n",
        "\n",
        "To use this Colab notebook, copy it to your own Google Drive and open it with [Colaboratory](https://colab.research.google.com/) (or Colab). To run a cell hold the Shift key and press the Enter key (or Return key). Colab automatically displays the return value of the last line in each cell. Refer to [this page](https://colab.sandbox.google.com/notebooks/welcome.ipynb) for more information on Colab.\n",
        "\n",
        "You can run a Colab notebook on a hosted runtime in the Cloud. The hosted VM times out after 90 minutes of inactivity and you will lose all the data stored in the memory including your authentication data. If your session gets disconnected (for example, because you closed your laptop) for less than the 90 minute inactivity timeout limit, press 'RECONNECT' on the top right corner of your notebook and resume the session. After Colab timeout, you'll need to\n",
        "\n",
        "1.   Re-run the initialization and authentication.\n",
        "2.   Continue from where you left off. You may need to copy-paste the value of some variables such as the `dataset_name` from the printed output of the previous cells.\n",
        "\n",
        "Alternatively you can connect your Colab notebook to a [local runtime](https://research.google.com/colaboratory/local-runtimes.html). \n",
        "It is recommended to run this notebook using vm, as the computational complexity is high enough that the hosted runtime becomes inconveniently slow. The local runtime link above also contains instructions for running the notebook on a vm. When using a vm, be sure to use a tensorflow vm, as it comes with the colab libraries. A standard vm of vCPUs will not work with the colab libraries required for this colab.\n"
      ]
    },
    {
      "metadata": {
        "id": "dMoTkf3BVD39",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#1. Project Set Up\n",
        "Follow the [AutoML Tables documentation](https://cloud.google.com/automl-tables/docs/) to\n",
        "* Create a Google Cloud Platform (GCP) project.\n",
        "* Enable billing.\n",
        "* Apply to whitelist your project.\n",
        "* Enable AutoML API.\n",
        "* Enable AutoML Tables API.\n",
        "* Create a service account, grant required permissions, and download the service account private key.\n",
        "\n",
        "You also need to upload your data into Google Cloud Storage (GCS) or BigQuery. For example, to use GCS as your data source\n",
        "* Create a GCS bucket.\n",
        "* Upload the training and batch prediction files.\n",
        "\n",
        "\n",
        "**Warning:** Private keys must be kept secret. If you expose your private key it is recommended to revoke it immediately from the Google Cloud Console.\n",
        "Extra steps, other than permission setting\n",
        "1. download both the client library and the service account\n",
        "2. zip up the client library and upload it to the vm\n",
        "3. upload the service account key to the vm\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "KAg-2-BQ4un6",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# 2. Initialize and authenticate\n",
        " This section runs intialization and authentication. It creates an authenticated session which is required for running any of the following sections."
      ]
    },
    {
      "metadata": {
        "id": "hid7SmtS4yE_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Install the client library\n",
        "Run the following cell to install uthe client library using pip."
      ]
    },
    {
      "metadata": {
        "id": "yXZlxqICsMg2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#@title Install AutoML Tables client library { vertical-output: true }\n",
        "!pip install google-cloud-automl",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "gGuRq4DI47hj",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Authenticate using service account key\n",
        "Create a service account key, and download it onto either your local machine or vm. Write in the path to the service account key. If your Service Account key file or folder is hidden, you can reveal it in a Mac by pressing the <b>Command + Shift + .</b> combo.\n"
      ]
    },
    {
      "metadata": {
        "id": "m3j1Kl4osNaJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#@title Authenticate using service account key and create a client. \n",
        "from google.cloud import automl_v1beta1\n",
        "import os \n",
        "path = \"my-project-trial5-e542e03e96c7.json\" #@param {type:'string'}\n",
        "os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = path\n",
        "\n",
        "# Authenticate and create an AutoML client.\n",
        "client = automl_v1beta1.AutoMlClient.from_service_account_file(path)\n",
        "# Authenticate and create a prediction service client.\n",
        "prediction_client = automl_v1beta1.PredictionServiceClient.from_service_account_file(path)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9zuplbargStJ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Enter your GCP project ID.\n"
      ]
    },
    {
      "metadata": {
        "id": "KIdmobtSsPj8",
        "colab_type": "code",
        "outputId": "14c234ca-5070-4301-a48c-c69d16ae4c31",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "#@title GCP project ID and location\n",
        "\n",
        "project_id = 'my-project-trial5' #@param {type:'string'}\n",
        "location = 'us-central1'\n",
        "location_path = client.location_path(project_id, location)\n",
        "location_path\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "e1fYDBjDgYEB",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# 3. Import, clean, transform and perform feature engineering on the training Data"
      ]
    },
    {
      "metadata": {
        "id": "dYoCTvaAgZK2",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Create dataset in AutoML Tables\n"
      ]
    },
    {
      "metadata": {
        "id": "uPRPqyw2gebp",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Select a dataset display name and pass your table source information to create a new dataset.\n"
      ]
    },
    {
      "metadata": {
        "id": "Iu3KNlcwsRhN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#@title Create dataset { vertical-output: true, output-height: 200 }\n",
        "\n",
        "dataset_display_name = 'colab_trial11' #@param {type: 'string'}\n",
        "\n",
        "create_dataset_response = client.create_dataset(\n",
        "    location_path,\n",
        "    {'display_name': dataset_display_name, 'tables_dataset_metadata': {}})\n",
        "dataset_name = create_dataset_response.name\n",
        "create_dataset_response\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "iTT5N97D0YPo",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Create a bucke to store the training data in"
      ]
    },
    {
      "metadata": {
        "id": "RQuGIbyGgud9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#@title Create bucket to store data in { vertical-output: true, output-height: 200 }\n",
        "\n",
        "bucket_name = 'trial_for_c4m' #@param {type: 'string'}\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "IQJuy1-PpF3b",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Import Dependencies\n"
      ]
    },
    {
      "metadata": {
        "id": "zzCeDmnnQRNy",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!sudo pip install google-cloud-bigquery google-cloud-storage pandas pandas-gbq gcsfs oauth2client\n",
        "\n",
        "import datetime\n",
        "import pandas as pd\n",
        "\n",
        "import gcsfs\n",
        "from google.cloud import bigquery\n",
        "from google.cloud import storage"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "UR5n1crIpQuX",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Transformation and Feature Engineering Functions\n"
      ]
    },
    {
      "metadata": {
        "id": "RODZJaq4o9b5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def balanceTable(table):\n",
        "\t#class count\n",
        "  count_class_false, count_class_true = table.totalTransactionRevenue.value_counts()\n",
        "\n",
        "\t#divide by class\n",
        "  table_class_false = table[table[\"totalTransactionRevenue\"] == False]\n",
        "  table_class_true = table[table[\"totalTransactionRevenue\"] == True]\n",
        "\n",
        "\t#random over-sampling\n",
        "  table_class_true_over = table_class_true.sample(count_class_false, replace = True)\n",
        "  table_test_over = pd.concat([table_class_false, table_class_true_over])\n",
        "  return table_test_over\n",
        "\n",
        "\n",
        "def partitionTable(table, dt=20170500):\n",
        "  #the automl tables model could be training on future data and implicitly learning about past data in the testing\n",
        "  #dataset, this would cause data leakage. To prevent this, we are training only with the first 9 months of data (table1)\n",
        "  #and doing validation with the last three months of data (table2).\n",
        "  table1 = table[table[\"date\"] <= dt]\n",
        "  table2 = table[table[\"date\"] > dt]\n",
        "  return table1, table2\n",
        "\n",
        "def N_updatePrevCount(table, new_column, old_column):\n",
        "  table = table.fillna(0)\n",
        "  table[new_column] = 1\n",
        "  table.sort_values(by=['fullVisitorId','date'])\n",
        "  table[new_column] = table.groupby(['fullVisitorId'])[old_column].apply(lambda x: x.cumsum())\n",
        "  table.drop([old_column], axis = 1, inplace = True)\n",
        "  return table\n",
        "\n",
        "\n",
        "def N_updateDate(table):\n",
        "  table['weekday'] = 1\n",
        "  table['date'] = pd.to_datetime(table['date'].astype(str), format = '%Y%m%d')\n",
        "  table['weekday'] = table['date'].dt.dayofweek\n",
        "  return table\n",
        "\n",
        "\n",
        "def change_transaction_values(table):\n",
        "  table['totalTransactionRevenue'] = table['totalTransactionRevenue'].fillna(0)\n",
        "  table['totalTransactionRevenue'] = table['totalTransactionRevenue'].apply(lambda x: x!=0)\n",
        "  return table\n",
        "\n",
        "def saveTable(table, csv_file_name, bucket_name):\n",
        "  table.to_csv(csv_file_name, index = False)\n",
        "  storage_client = storage.Client()\n",
        "  bucket = storage_client.get_bucket(bucket_name)\n",
        "  blob = bucket.blob(csv_file_name)\n",
        "  blob.upload_from_filename(filename = csv_file_name)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2eGAIUmRqjqX",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "###Import training data"
      ]
    },
    {
      "metadata": {
        "id": "XTmXPMUsTgEs",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "You also have the option of just downloading the file, FULL.csv, [here](https://storage.cloud.google.com/cloud-ml-data/automl-tables/notebooks/trial_for_c4m/FULL.csv), instead of running the code below. Just be sure to move the file into the google cloud storage bucket you specified above."
      ]
    },
    {
      "metadata": {
        "id": "Bl9-DSjIqj7c",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#@title Input name of file to save data to { vertical-output: true, output-height: 200 }\n",
        "sqll = '''\n",
        "SELECT\n",
        "date, device, geoNetwork, totals, trafficSource, fullVisitorId \n",
        "FROM `bigquery-public-data.google_analytics_sample.ga_sessions_*`\n",
        "WHERE\n",
        "_TABLE_SUFFIX BETWEEN FORMAT_DATE('%Y%m%d',DATE_SUB('2017-08-01', INTERVAL 366 DAY))\n",
        "AND\n",
        "FORMAT_DATE('%Y%m%d',DATE_SUB('2017-08-01', INTERVAL 1 DAY))\n",
        "'''\n",
        "df = pd.read_gbq(sqll, project_id = project_id, dialect='standard')\n",
        "print(df.iloc[:3])\n",
        "path_to_data_pre_transformation = \"FULL.csv\" #@param {type: 'string'}\n",
        "saveTable(df, path_to_data_pre_transformation, bucket_name)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "V5WK71tiq-2b",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "###Unnest the Data"
      ]
    },
    {
      "metadata": {
        "id": "RFpgLfeNqUBk",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#some transformations on the basic dataset\n",
        "#@title Input the name of file to hold the unnested data to { vertical-output: true, output-height: 200 }\n",
        "unnested_file_name = \"FULL_unnested.csv\" #@param {type: 'string'}\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2dyJlNAVqXUn",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "You also have the option of just downloading the file, FULL_unnested.csv, [here](https://storage.cloud.google.com/cloud-ml-data/automl-tables/notebooks/trial_for_c4m/FULL_unnested.csv), instead of running the code below. Just be sure to move the file into the google cloud storage bucket you specified above."
      ]
    },
    {
      "metadata": {
        "id": "tLPHeF2Y2l5l",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "table = pd.read_csv(\"gs://\"+bucket_name+\"/\"+file_name)\n",
        "\n",
        "column_names = ['device', 'geoNetwork','totals', 'trafficSource']\n",
        "\n",
        "for name in column_names:\n",
        "  print(name)\n",
        "  table[name] = table[name].apply(lambda i: dict(eval(i)))\n",
        "  temp = table[name].apply(pd.Series)\n",
        "  table = pd.concat([table, temp], axis=1).drop(name, axis=1)\n",
        "\n",
        "#need to drop a column\n",
        "table.drop(['adwordsClickInfo'], axis = 1, inplace = True)\n",
        "saveTable(table, unnested_file_name, bucket_name)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9_WC-AJLsdqo",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "###Run the Transformations"
      ]
    },
    {
      "metadata": {
        "id": "YWQ4462vnpOg",
        "colab_type": "code",
        "outputId": "5ca7e95a-e0f2-48c2-9b59-8f043d233bd2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        }
      },
      "cell_type": "code",
      "source": [
        "table = pd.read_csv(\"gs://\"+bucket_name+\"/\"+unnested_file_name)\n",
        "\n",
        "consts = ['transactionRevenue', 'transactions', 'adContent', 'browserSize', 'campaignCode', \n",
        "'cityId', 'flashVersion', 'javaEnabled', 'language', 'latitude', 'longitude', 'mobileDeviceBranding', \n",
        "'mobileDeviceInfo', 'mobileDeviceMarketingName','mobileDeviceModel','mobileInputSelector', 'networkLocation', \n",
        "'operatingSystemVersion', 'screenColors', 'screenResolution', 'screenviews', 'sessionQualityDim', 'timeOnScreen',\n",
        "'visits', 'uniqueScreenviews', 'browserVersion','referralPath','fullVisitorId', 'date']\n",
        "\n",
        "table = N_updatePrevCount(table, 'previous_views', 'pageviews')\n",
        "table = N_updatePrevCount(table, 'previous_hits', 'hits')\n",
        "table = N_updatePrevCount(table, 'previous_timeOnSite', 'timeOnSite')\n",
        "table = N_updatePrevCount(table, 'previous_Bounces', 'bounces')\n",
        "\n",
        "table = change_transaction_values(table)\n",
        "\n",
        "table1, table2 = partitionTable(table)\n",
        "table1 = N_updateDate(table1)\n",
        "table2 = N_updateDate(table2)\n",
        "#validation_unnested_FULL.csv = the last 3 months of data\n",
        "\n",
        "table.drop(consts, axis = 1, inplace = True)\n",
        "\n",
        "saveTable(table2,'validation_unnested_FULL.csv', bucket_name)\n",
        "\n",
        "table1 = balanceTable(table1)\n",
        "\n",
        "#training_unnested_FULL.csv = the first 9 months of data\n",
        "saveTable(table1, 'training_unnested_balanced_FULL.csv', bucket_name)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "LqmARBnRHWh8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#@title ... take the data source from GCS { vertical-output: true } \n",
        "\n",
        "dataset_gcs_input_uris = ['gs://trial_for_c4m/training_unnested_balanced_FULL.csv',] #@param\n",
        "# Define input configuration.\n",
        "input_config = {\n",
        "    'gcs_source': {\n",
        "        'input_uris': dataset_gcs_input_uris\n",
        "    }\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "SfXjtAwDsYlV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        " #@title Import data { vertical-output: true }\n",
        "\n",
        "import_data_response = client.import_data(dataset_name, input_config)\n",
        "print('Dataset import operation: {}'.format(import_data_response.operation))\n",
        "# Wait until import is done.\n",
        "import_data_result = import_data_response.result()\n",
        "import_data_result"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "W3SiSLS4tml9",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# 4. Update dataset: assign a label column and enable nullable columns"
      ]
    },
    {
      "metadata": {
        "id": "jVo8Z8PGtpB7",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "AutoML Tables automatically detects your data column type. Depending on the type of your label column, AutoML Tables chooses to run a classification or regression model. If your label column contains only numerical values, but they represent categories, change your label column type to categorical by updating your schema."
      ]
    },
    {
      "metadata": {
        "id": "dMdOoFsXxyxj",
        "colab_type": "code",
        "outputId": "e6fab957-2316-48c0-be66-1bff9dc5c23c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        }
      },
      "cell_type": "code",
      "source": [
        "#@title Table schema { vertical-output: true }\n",
        "\n",
        "import google.cloud.automl_v1beta1.proto.data_types_pb2 as data_types\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# List table specs\n",
        "list_table_specs_response = client.list_table_specs(dataset_name)\n",
        "table_specs = [s for s in list_table_specs_response]\n",
        "# List column specs\n",
        "table_spec_name = table_specs[0].name\n",
        "list_column_specs_response = client.list_column_specs(table_spec_name)\n",
        "column_specs = {s.display_name: s for s in list_column_specs_response}\n",
        "# Table schema pie chart.\n",
        "type_counts = {}\n",
        "for column_spec in column_specs.values():\n",
        "  type_name = data_types.TypeCode.Name(column_spec.data_type.type_code)\n",
        "  type_counts[type_name] = type_counts.get(type_name, 0) + 1\n",
        "\n",
        "plt.pie(x=type_counts.values(), labels=type_counts.keys(), autopct='%1.1f%%')\n",
        "plt.axis('equal')\n",
        "plt.show()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "AfT4upKysamH",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#@title Update a column: set to not nullable { vertical-output: true }\n",
        "\n",
        "update_column_spec_dict = {\n",
        "    'name': column_specs['totalTransactionRevenue'].name,\n",
        "    'data_type': {\n",
        "        'type_code': 'CATEGORY',\n",
        "        'nullable': False\n",
        "    }\n",
        "}\n",
        "update_column_response = client.update_column_spec(update_column_spec_dict)\n",
        "update_column_response"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3O9cFko3t3ai",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Tip:** You can use `'type_code': 'CATEGORY'` in the preceding `update_column_spec_dict` to convert the column data type from `FLOAT64` `to `CATEGORY`.\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "rR2RaPP7t6y8",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Update dataset: assign a label"
      ]
    },
    {
      "metadata": {
        "id": "aTt2mIzbsduV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#@title Update dataset { vertical-output: true }\n",
        "\n",
        "label_column_name = 'totalTransactionRevenue' #@param {type: 'string'}\n",
        "label_column_spec = column_specs[label_column_name]\n",
        "label_column_id = label_column_spec.name.rsplit('/', 1)[-1]\n",
        "print('Label column ID: {}'.format(label_column_id))\n",
        "# Define the values of the fields to be updated.\n",
        "update_dataset_dict = {\n",
        "    'name': dataset_name,\n",
        "    'tables_dataset_metadata': {\n",
        "        'target_column_spec_id': label_column_id\n",
        "    }\n",
        "}\n",
        "update_dataset_response = client.update_dataset(update_dataset_dict)\n",
        "update_dataset_response"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xajewSavt9K1",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#5. Creating a model"
      ]
    },
    {
      "metadata": {
        "id": "dA-FE6iWt-A_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Train a model\n",
        "Training the model may take one hour or more. The following cell keeps running until the training is done. If your Colab times out, use `client.list_models(location_path)` to check whether your model has been created. Then use model name to continue to the next steps. Run the following command to retrieve your model. Replace `model_name` with its actual value.\n",
        "\n",
        "    model = client.get_model(model_name)\n",
        "  "
      ]
    },
    {
      "metadata": {
        "id": "Kp0gGkp8H3zj",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#@title Create model { vertical-output: true }\n",
        "#this will create a model that can be access through the auto ml tables colab\n",
        "model_display_name = 'trial_10' #@param {type:'string'}\n",
        "\n",
        "model_dict = {\n",
        "    'display_name': model_display_name,\n",
        "    'dataset_id': dataset_name.rsplit('/', 1)[-1],\n",
        "    'tables_model_metadata': {'train_budget_milli_node_hours': 1000}\n",
        "}\n",
        "create_model_response = client.create_model(location_path, model_dict)\n",
        "print('Dataset import operation: {}'.format(create_model_response.operation))\n",
        "# Wait until model training is done.\n",
        "create_model_result = create_model_response.result()\n",
        "model_name = create_model_result.name\n",
        "print(model_name)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tCIk1e4UuDxZ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# 6. Make a prediction"
      ]
    },
    {
      "metadata": {
        "id": "H7Fi5f9zuG5f",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "There are two different prediction modes: online and batch. The following cell shows you how to make a batch prediction."
      ]
    },
    {
      "metadata": {
        "id": "AZ_CPff77m4e",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#@title Start batch prediction { vertical-output: true, output-height: 200 }\n",
        "print(client.list_models(location_path))\n",
        "\n",
        "batch_predict_gcs_input_uris = ['gs://trial_for_c4m/validation_unnested_FULL.csv',] #@param\n",
        "batch_predict_gcs_output_uri_prefix = 'gs://trial_for_c4m' #@param {type:'string'}\n",
        "# Define input source.\n",
        "batch_prediction_input_source = {\n",
        "  'gcs_source': {\n",
        "    'input_uris': batch_predict_gcs_input_uris\n",
        "  }\n",
        "}\n",
        "# Define output target.\n",
        "batch_prediction_output_target = {\n",
        "    'gcs_destination': {\n",
        "      'output_uri_prefix': batch_predict_gcs_output_uri_prefix\n",
        "    }\n",
        "}\n",
        "batch_predict_response = prediction_client.batch_predict(\n",
        "    model_name, batch_prediction_input_source, batch_prediction_output_target)\n",
        "print('Batch prediction operation: {}'.format(batch_predict_response.operation))\n",
        "# Wait until batch prediction is done.\n",
        "batch_predict_result = batch_predict_response.result()\n",
        "batch_predict_response.metadata\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "utGPmXI-uKNr",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#7. Evaluate your prediction"
      ]
    },
    {
      "metadata": {
        "id": "GsOdhJeauTC3",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The follow cell creates a Precision Recall Curve and a ROC curve for both the true and false classifications.\n",
        "Fill in the batch_predict_results_location with the location of the results.csv file created in the previous \"Make a prediction\" step\n"
      ]
    },
    {
      "metadata": {
        "id": "orejkh0CH4mu",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "import numpy as np\n",
        "from sklearn import metrics\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def invert(x):\n",
        "  return 1-x\n",
        "\n",
        "def switch_label(x):\n",
        "  return(not x)\n",
        "batch_predict_results_location = 'gs://trial_for_c4m/prediction-trial_10-2019-03-23T00:22:56.802Z' #@param {type:'string'}\n",
        "\n",
        "table = pd.read_csv(batch_predict_results_location +'/result.csv')\n",
        "y = table[\"totalTransactionRevenue\"]\n",
        "scores = table[\"totalTransactionRevenue_1.0_score\"]\n",
        "scores_invert = table['totalTransactionRevenue_0.0_score']\n",
        "\n",
        "#code for ROC curve, for true values\n",
        "fpr, tpr, thresholds = metrics.roc_curve(y, scores)\n",
        "roc_auc = metrics.auc(fpr, tpr)\n",
        "\n",
        "plt.figure()\n",
        "lw = 2\n",
        "plt.plot(fpr, tpr, color='darkorange',\n",
        "         lw=lw, label='ROC curve (area = %0.2f)' % roc_auc)\n",
        "plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('Receiver operating characteristic for True')\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.show()\n",
        "\n",
        "\n",
        "#code for ROC curve, for false values\n",
        "plt.figure()\n",
        "lw = 2\n",
        "label_invert = y.apply(switch_label)\n",
        "fpr, tpr, thresholds = metrics.roc_curve(label_invert, scores_invert)\n",
        "plt.plot(fpr, tpr, color='darkorange',\n",
        "         lw=lw, label='ROC curve (area = %0.2f)' % roc_auc)\n",
        "plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('Receiver operating characteristic for False')\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.show()\n",
        "\n",
        "\n",
        "#code for PR curve, for true values\n",
        "\n",
        "precision, recall, thresholds = metrics.precision_recall_curve(y, scores)\n",
        "\n",
        "\n",
        "plt.figure()\n",
        "lw = 2\n",
        "plt.plot( recall, precision, color='darkorange',\n",
        "         lw=lw, label='Precision recall curve for True')\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('Recall')\n",
        "plt.ylabel('Precision')\n",
        "plt.title('Precision Recall Curve for True')\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.show()\n",
        "\n",
        "#code for PR curve, for false values\n",
        "\n",
        "precision, recall, thresholds = metrics.precision_recall_curve(label_invert, scores_invert)\n",
        "print(precision.shape)\n",
        "print(recall.shape)\n",
        "\n",
        "plt.figure()\n",
        "lw = 2\n",
        "plt.plot( recall, precision, color='darkorange',\n",
        "          label='Precision recall curve for False')\n",
        "plt.xlim([0.0, 1.1])\n",
        "plt.ylim([0.0, 1.1])\n",
        "plt.xlabel('Recall')\n",
        "plt.ylabel('Precision')\n",
        "plt.title('Precision Recall Curve for False')\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.show()\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
